# databricks-job-monitoring

# ğŸ“Š Monitoramento de Performance de Jobs no Databricks

## ğŸ“Œ DescriÃ§Ã£o do Projeto
Este projeto tem como objetivo coletar logs de execuÃ§Ã£o dos jobs no **Databricks**, processÃ¡-los e armazenÃ¡-los para futuras anÃ¡lises. A ideia Ã© criar um pipeline de monitoramento que permita entender o desempenho dos jobs e detectar falhas ou lentidÃ£o.

## ğŸš€ Tecnologias Utilizadas
- **Python** â†’ Para extrair e processar os logs da API do Databricks.
- **Databricks API** â†’ Para obter informaÃ§Ãµes dos jobs executados.
- **Pandas** â†’ Para manipulaÃ§Ã£o dos dados extraÃ­dos.
- **Parquet** â†’ Para armazenamento otimizado dos logs.

## ğŸ“‚ Estrutura do Projeto
```
ğŸ“ monitoramento-jobs-databricks
â”‚â”€â”€ ğŸ“„ databricks_logs_extraction.py  # Script para coletar logs da API
â”‚â”€â”€ ğŸ“„ README.md  # DocumentaÃ§Ã£o do projeto
```

## ğŸ”§ Como Executar o Script
1. **Configurar as credenciais do Databricks**
   - No arquivo `databricks_logs_extraction.py`, substitua:
     ```python
     DATABRICKS_URL = "https://<seu-workspace>.cloud.databricks.com"
     DATABRICKS_TOKEN = "<seu-token-aqui>"
     ```
   - Coloque o **Databricks Workspace URL** e o **Token de AutenticaÃ§Ã£o**.

2. **Instalar as dependÃªncias**
   Se ainda nÃ£o tiver instalado, execute:
   ```bash
   pip install requests pandas pyarrow
   ```

3. **Rodar o script**
   ```bash
   python databricks_logs_extraction.py
   ```

4. **Verificar a saÃ­da**
   - O script gerarÃ¡ um arquivo `databricks_logs.parquet` contendo os logs extraÃ­dos.

## ğŸ“Œ PrÃ³ximos Passos (AC2, AC3 e Entrega Final)
- **AC2 (06/04):** Criar pipeline de processamento e estruturaÃ§Ã£o dos dados.
- **AC3 (04/05):** Criar dashboard e sistema de alertas para falhas.
- **Entrega Final (08/06):** Refinar o cÃ³digo e documentaÃ§Ã£o final do projeto.

## ğŸ“ Links Importantes
- **[RepositÃ³rio no GitHub](#)**
- **[Board do GitHub Projects](#)** 
